# -*- coding: utf-8 -*-
"""Simple_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sCkP_joEoM5qPXX-DAs6kaWIL9Atth8c
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load the Llama model and tokenizer
model_id = "meta-llama/Llama-3.2-1B"  # Or your desired Llama model
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# Save the model's state dictionary
torch.save(model.state_dict(), "llama_weights.pth")

model.load_state_dict(torch.load("llama_weights.pth"))
model.eval()

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
import gc
import os

# Set environment variables to help with memory fragmentation
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

class SimpleLLM(nn.Module):
    def __init__(self, model_id="meta-llama/Llama-3.2-1B"):
        super().__init__()

        # Clear memory before loading
        self._clear_memory()

        print("Loading tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)

        print("Loading model...")
        hf_model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            device_map="auto"
        )

        # Save config
        self.config = hf_model.config

        print("Extracting weights...")
        self._extract_and_initialize(hf_model)

        # Free up memory
        del hf_model
        self._clear_memory()

    def _clear_memory(self):
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def _extract_and_initialize(self, hf_model):
        print("Initializing embedding layer...")
        self.embed_tokens = nn.Embedding(self.config.vocab_size, self.config.hidden_size)
        self.embed_tokens.weight.data.copy_(hf_model.model.embed_tokens.weight.data)

        print("Initializing transformer blocks...")
        self.layers = nn.ModuleList()
        for i in range(self.config.num_hidden_layers):
            layer = self._create_transformer_block(i, hf_model)
            self.layers.append(layer)
            self._clear_memory()

        print("Initializing final layers...")
        self.norm = nn.LayerNorm(self.config.hidden_size, eps=self.config.rms_norm_eps)
        self.norm.weight.data.copy_(hf_model.model.norm.weight.data)

        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)
        self.lm_head.weight.data.copy_(hf_model.lm_head.weight.data)

    def _create_transformer_block(self, layer_idx, hf_model):
        hf_layer = hf_model.model.layers[layer_idx]
        block = nn.ModuleDict()

        # Get shapes directly from HF model
        q_weight = hf_layer.self_attn.q_proj.weight
        k_weight = hf_layer.self_attn.k_proj.weight
        v_weight = hf_layer.self_attn.v_proj.weight
        o_weight = hf_layer.self_attn.o_proj.weight

        # Attention components with correct dimensions
        block['attention'] = nn.ModuleDict({
            'q_proj': nn.Linear(self.config.hidden_size, q_weight.size(0), bias=False),
            'k_proj': nn.Linear(self.config.hidden_size, k_weight.size(0), bias=False),
            'v_proj': nn.Linear(self.config.hidden_size, v_weight.size(0), bias=False),
            'o_proj': nn.Linear(o_weight.size(1), self.config.hidden_size, bias=False)
        })

        # Copy attention weights
        block['attention']['q_proj'].weight.data.copy_(q_weight.data)
        block['attention']['k_proj'].weight.data.copy_(k_weight.data)
        block['attention']['v_proj'].weight.data.copy_(v_weight.data)
        block['attention']['o_proj'].weight.data.copy_(o_weight.data)

        # MLP components
        gate_weight = hf_layer.mlp.gate_proj.weight
        down_weight = hf_layer.mlp.down_proj.weight
        up_weight = hf_layer.mlp.up_proj.weight

        block['mlp'] = nn.ModuleDict({
            'gate_proj': nn.Linear(self.config.hidden_size, gate_weight.size(0), bias=False),
            'up_proj': nn.Linear(self.config.hidden_size, up_weight.size(0), bias=False),
            'down_proj': nn.Linear(gate_weight.size(0), self.config.hidden_size, bias=False)
        })

        # Copy MLP weights
        block['mlp']['gate_proj'].weight.data.copy_(gate_weight.data)
        block['mlp']['up_proj'].weight.data.copy_(up_weight.data)
        block['mlp']['down_proj'].weight.data.copy_(down_weight.data)

        # Layer norms
        block['input_layernorm'] = nn.LayerNorm(
            self.config.hidden_size,
            eps=self.config.rms_norm_eps
        )
        block['post_attention_layernorm'] = nn.LayerNorm(
            self.config.hidden_size,
            eps=self.config.rms_norm_eps
        )

        # Copy layer norm weights
        block['input_layernorm'].weight.data.copy_(hf_layer.input_layernorm.weight.data)
        block['post_attention_layernorm'].weight.data.copy_(hf_layer.post_attention_layernorm.weight.data)

        return block

    def forward(self, input_ids, past_key_values=None):
        """Compute logits for a given input sequence using only PyTorch operations."""
        batch_size, seq_length = input_ids.shape
        device = input_ids.device

        # Get token embeddings
        hidden_states = self.embed_tokens(input_ids)

        # Process through transformer layers
        for layer in self.layers:
            # Apply input layer norm
            residual = hidden_states
            hidden_states = layer['input_layernorm'](hidden_states)

            # Self-attention
            q = layer['attention']['q_proj'](hidden_states)
            k = layer['attention']['k_proj'](hidden_states)
            v = layer['attention']['v_proj'](hidden_states)

            # Get dimensions for attention
            q_size = q.size(-1)
            k_size = k.size(-1)
            v_size = v.size(-1)

            # Calculate number of heads based on dimensions
            num_q_heads = self.config.num_attention_heads
            head_dim = q_size // num_q_heads
            num_kv_heads = k_size // head_dim

            # Reshape for multi-head attention
            q = q.view(batch_size, seq_length, num_q_heads, head_dim).transpose(1, 2)
            k = k.view(batch_size, seq_length, num_kv_heads, head_dim).transpose(1, 2)
            v = v.view(batch_size, seq_length, num_kv_heads, head_dim).transpose(1, 2)

            # Ensure k and v have the same number of heads as q
            if num_q_heads != num_kv_heads:
                k = k.repeat_interleave(num_q_heads // num_kv_heads, dim=1)
                v = v.repeat_interleave(num_q_heads // num_kv_heads, dim=1)

            # Compute attention scores
            attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (head_dim ** 0.5)

            # Apply causal mask
            causal_mask = torch.triu(
                torch.ones(seq_length, k.size(2), device=device) * float('-inf'),
                diagonal=1
            )
            attn_weights = attn_weights + causal_mask.unsqueeze(0).unsqueeze(0)

            # Apply softmax
            attn_weights = F.softmax(attn_weights, dim=-1)

            # Apply attention to values
            context = torch.matmul(attn_weights, v)
            context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, q_size)

            # Output projection
            attn_output = layer['attention']['o_proj'](context)

            # Residual connection
            hidden_states = residual + attn_output

            # Feed-forward network
            residual = hidden_states
            hidden_states = layer['post_attention_layernorm'](hidden_states)

            # SwiGLU activation
            gate_output = F.silu(layer['mlp']['gate_proj'](hidden_states))
            up_output = layer['mlp']['up_proj'](hidden_states)
            mlp_output = gate_output * up_output
            mlp_output = layer['mlp']['down_proj'](mlp_output)

            hidden_states = residual + mlp_output

        # Final layer norm
        hidden_states = self.norm(hidden_states)

        # Get logits
        logits = self.lm_head(hidden_states)

        return logits, past_key_values

    @torch.inference_mode()
    def generate(self, prompt, max_length=512, num_beams=5, early_stopping=True, no_repeat_ngram_size=2, temperature=0.7, top_k=40, top_p=0.9, repetition_penalty=1.2):
        """Generate text token-by-token using beam search with PyTorch operations."""
        if isinstance(prompt, str):
            input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids
        else:
            input_ids = prompt

        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        input_ids = input_ids.to(device)
        self.to(device)

        # Initialize beam search
        batch_size = 1
        vocab_size = self.config.vocab_size

        # Start with prompt
        beam_scores = torch.zeros((batch_size * num_beams), device=device)
        beam_tokens = input_ids.repeat(num_beams, 1)
        beam_indices = torch.arange(num_beams, device=device)

        # Track finished sentences
        done = [False for _ in range(batch_size * num_beams)]

        # Generate tokens
        for step in range(max_length - input_ids.size(1)):
            # Forward pass
            logits, _ = self.forward(beam_tokens)
            next_token_logits = logits[:, -1, :]

            # Apply temperature and repetition penalty
            next_token_logits = next_token_logits / temperature
            for i in range(batch_size * num_beams):
                for token_id in beam_tokens[i]:
                    next_token_logits[i, token_id.item()] /= repetition_penalty

            # Apply top-k and top-p filtering
            if top_k > 0:
                # Keep only top k tokens
                values, _ = torch.topk(next_token_logits, top_k)
                min_values = values[:, -1].unsqueeze(1).repeat(1, next_token_logits.shape[-1])
                next_token_logits = torch.where(
                    next_token_logits < min_values,
                    torch.full_like(next_token_logits, float('-inf')),
                    next_token_logits
                )

            if top_p < 1.0:
                # Nucleus sampling
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

                # Remove tokens with cumulative probability above the threshold
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
                sorted_indices_to_remove[:, 0] = 0

                # Create indices to remove
                indices_to_remove = sorted_indices_to_remove.scatter(
                    1, sorted_indices, sorted_indices_to_remove
                )
                next_token_logits = next_token_logits.masked_fill(indices_to_remove, float('-inf'))

            # Calculate log probabilities
            next_scores = F.log_softmax(next_token_logits, dim=-1)

            # Add beam scores
            next_scores = next_scores + beam_scores[:, None]

            # Reshape for beam search
            next_scores = next_scores.view(batch_size, num_beams * vocab_size)

            # Get top-k scores and indices
            next_scores, next_tokens = torch.topk(
                next_scores, num_beams, dim=1, largest=True, sorted=True
            )

            # Get beam indices and token indices
            next_beam_tokens = next_tokens % vocab_size
            next_beam_indices = next_tokens // vocab_size

            # Create new beams
            new_beam_tokens = []
            new_beam_scores = []

            for batch_idx in range(batch_size):
                for beam_idx in range(num_beams):
                    token_id = next_beam_tokens[batch_idx, beam_idx]
                    beam_idx = next_beam_indices[batch_idx, beam_idx]

                    # Add to new beam
                    new_beam = torch.cat([beam_tokens[beam_idx], token_id.unsqueeze(0)], dim=0)
                    new_beam_tokens.append(new_beam)
                    new_beam_scores.append(next_scores[batch_idx, beam_idx])

            # Update beams
            beam_tokens = torch.stack(new_beam_tokens)
            beam_scores = torch.stack(new_beam_scores)

            # Early stopping
            if early_stopping and (beam_tokens[:, -1] == self.tokenizer.eos_token_id).any():
                break

        # Decode generated tokens
        generated_texts = [self.tokenizer.decode(tokens, skip_special_tokens=True) for tokens in beam_tokens]

        # Clear memory
        self._clear_memory()

        return generated_texts

# Example usage
if __name__ == "__main__":
    # Create the model
    model = SimpleLLM(model_id="meta-llama/Llama-3.2-1B")

    # Generate text with beam search
    generated_texts = model.generate(
        "Every effort moves you",
        max_length=100,
        num_beams=1,
        early_stopping=True,
        no_repeat_ngram_size=2,
        temperature=0.7,
        top_k=40,
        top_p=0.9,
        repetition_penalty=1.2
    )
    for i, text in enumerate(generated_texts):
        print(f"Beam {i+1}:\n", text)
        print('-' * 50)

